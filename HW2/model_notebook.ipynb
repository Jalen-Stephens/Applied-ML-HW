{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Museum Image Model â€” Data, CNN & Embeddings\n",
    "\n",
    "This notebook splits the `model.py` script into sections: data loading, transforms, visualization, CNN definition, and embedding extraction with PCA/t-SNE/UMAP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class MuseumDataset(Dataset):\n",
    "    \"\"\"Custom dataset: images + CSV metadata.\"\"\"\n",
    "    def __init__(self, image_dir, csv_path, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.metadata = pd.read_csv(csv_path)\n",
    "        self.image_files = sorted(os.listdir(image_dir))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transforms & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  # ImageNet stats\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "train_image_dir = \"HW2/data_oceania_HW2/train\"\n",
    "train_csv_path  = \"HW2/data_oceania_HW2/metadata.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MuseumDataset(train_image_dir, train_csv_path, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_batch(loader, n=8):\n",
    "    images, idxs = next(iter(loader))\n",
    "    fig, axes = plt.subplots(1, min(n, len(images)), figsize=(2*n, 2))\n",
    "    for i in range(min(n, len(images))):\n",
    "        img = images[i].permute(1, 2, 0)\n",
    "        img = img * torch.tensor([0.229, 0.224, 0.225]) + \\\n",
    "                     torch.tensor([0.485, 0.456, 0.406])\n",
    "        img = img.clamp(0, 1)\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_batch(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build Random CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "num_layers = np.random.randint(2, 6)\n",
    "channels = [3] + [np.random.choice([16, 32, 64]) for _ in range(num_layers)]\n",
    "\n",
    "layers = []\n",
    "for i in range(num_layers):\n",
    "    layers += [\n",
    "        nn.Conv2d(\n",
    "            in_channels=channels[i],\n",
    "            out_channels=channels[i+1],\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        ),\n",
    "        nn.BatchNorm2d(channels[i+1]),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2)\n",
    "    ]\n",
    "\n",
    "layers.append(nn.AdaptiveAvgPool2d(1))\n",
    "layers.append(nn.Flatten())\n",
    "\n",
    "random_cnn = nn.Sequential(*layers)\n",
    "\n",
    "# Test it\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "embedding = random_cnn(x)\n",
    "print(f\"Layers: {num_layers}, Embedding dim: {embedding.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extract Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "\n",
    "embeddings_list = []\n",
    "indices_list = []\n",
    "\n",
    "random_cnn.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "random_cnn = random_cnn.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, idxs in train_loader:\n",
    "        images = images.to(device)\n",
    "        emb = random_cnn(images)\n",
    "        embeddings_list.append(emb.cpu())\n",
    "        indices_list.append(idxs)\n",
    "\n",
    "embeddings = torch.cat(embeddings_list, dim=0).numpy()\n",
    "all_idxs    = torch.cat(indices_list, dim=0).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dimensionality Reduction & Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = train_dataset.metadata\n",
    "\n",
    "# Change to: 'artist_culture', 'region', or 'medium_materials'\n",
    "color_col = \"artist_culture\"\n",
    "\n",
    "label_strings = metadata[color_col].iloc[all_idxs].astype(str).values\n",
    "unique_labels = list(dict.fromkeys(label_strings))\n",
    "color_ids = np.array([unique_labels.index(l) for l in label_strings])\n",
    "\n",
    "pca_2d  = PCA(n_components=2).fit_transform(embeddings)\n",
    "tsne_2d = TSNE(n_components=2, perplexity=30, init=\"random\", learning_rate=\"auto\").fit_transform(embeddings)\n",
    "umap_2d = umap.UMAP(n_components=2).fit_transform(embeddings)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, data, title in zip(\n",
    "    axes,\n",
    "    [pca_2d, tsne_2d, umap_2d],\n",
    "    [\"PCA\", \"t-SNE\", \"UMAP\"]\n",
    "):\n",
    "    scatter = ax.scatter(data[:, 0], data[:, 1], c=color_ids, cmap=\"tab10\", s=8, alpha=0.7)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "plt.colorbar(scatter, ax=axes[-1])\n",
    "plt.suptitle(f\"Colored by: {color_col}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
